import requests
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup

def get_http_response(url: str):
    """
    Gets HTTP response with Accept and User-Agent headers.
    """
    
    headers = {
        'Accept': 'text/html,application/xhtml+xml,application/xml',
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0'
    }
    response = requests.get(url, headers=headers)
    return response

def get_XML_root_from_url(url:str):
    """
    Takes in URL of XML Sitemap.
    Returns the XML root object.
    """

    # Only allow xml files
    if not url.endswith('.xml'):
        return []

    # Retrieve XML root
    headers = {
        'Accept': 'application/xhtml+xml,application/xml',
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0'
    }
    response = requests.get(url, headers=headers)
    xml_data = response.content
    root = ET.fromstring(xml_data)

    return root

def get_links_from_xml_url(url) -> list:
    """
    Takes in XML root object.
    Returns a list of all URLs in XML sitemap.
    """
    # Retrieve XML root
    root = get_XML_root_from_url(url)

    # Retrieve XML links
    links = []
    for child in root:
        for subchild in child:
            if "loc" in subchild.tag and 'http' in subchild.text:
                links.append(subchild.text)
    
    return links


def get_links_from_yoast(url: str):
    """
    Takes in a YoastSEO XML Sitemap file via URL
    Returns list of links from the XML Sitemap

    Format: https://example.com/sitemap.xml or https://example.com/sitemap_index.xml
    """

    # Pick up XML links
    xml_links = get_links_from_xml_url(url)
    
    # Parse through each XML link picked up on the first run.
    #
    # Assumption 1: The links picked up on the first run are additional 
    #               XML sitemap links generated by Yoast
    # Assumption 2: There are no additional XML files in the sitemap tree.
    site_links = []
    for link in xml_links:
        if link.endswith('.xml'):
            for new_link in get_links_from_xml_url(link):
                site_links.append(new_link)
    
    del xml_links
    
    return site_links

def get_headings_from_content(soup):
    """
    Takes in BeautifulSoup object
    Returns dictionary of all headings in soup object.
    """
    headings = {
        'h1': [],
        'h2': [],
        'h3': [],
        'h4': [],
        'h5': [],
        'h6': []
    }

    for heading_tag in headings.keys():
        results = soup.find_all(heading_tag)
        for heading in results:
            headings[heading_tag].append(heading.text)
    
    return headings

def get_anchor_hrefs_from_content(soup):
    """
    Retrieves all href attributes from all anchor tags in url.
    """
    anchor_tags = soup.find_all('a')
    hrefs = []
    for tag in anchor_tags:
        href = tag.get('href')
        if href:
            hrefs.append(href)
    return hrefs

def get_img_info_from_content(soup):
    """
    Takes in BeautifulSoup object
    Creates a dictionary object for each image on webpage
    Returns a list of those dictionary objects
    """
    images = soup.find_all('img')

    img_list = []

    for img in images:
        try:
            alt_text = img.get('alt')
        except:
            alt_text = ""
        
        try:
            src = img.get('src')
            img_dict = {
                'alt': alt_text,
                'src': src
            }
            img_list.append(img_dict)
        except:
            continue
    
    return img_list

def get_content_object(url):
    """
    Creates a dictionary object of HTML content
    """
    response = get_http_response(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    headings = get_headings_from_content(soup)
    hrefs = get_anchor_hrefs_from_content(soup)
    images = get_img_info_from_content(soup)

    return {
        'headings': headings,
        'hrefs': hrefs,
        'images': images
    }

